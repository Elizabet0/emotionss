{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "358b20a8-1b04-4e12-8389-58603d929829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95aec045-f818-4286-a645-0fda3fe3b942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitry\\AppData\\Local\\Temp\\ipykernel_36164\\1285168252.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"../best_models/model_checkpoint_60.62.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x192361c4a10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),  \n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),  \n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Flatten(), \n",
    "            nn.Dropout(0.5), \n",
    "            nn.Linear(64 * 6 * 6, 256), nn.BatchNorm1d(256), nn.ReLU(), \n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 7), \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "checkpoint = torch.load(\"../best_models/model_checkpoint_60.62.pth\")\n",
    "model = CNN()\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "classes = list(os.listdir('../data/train'))\n",
    "\n",
    "catboost_model = CatBoostClassifier()\n",
    "catboost_model.load_model('../best_models/catboost_model.cbm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0cbbc1-79f3-46ef-b1fc-a3e485e5dd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ed2f3d7-77b7-4594-b459-a2061b05b7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAwADABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AOtWxH8LkfUZpLnZp1s1xdSxxxL1Yn+lQw67pw06S+B81YzzEWVGYeoyf0qW31zSdWg+0ae0iBeHjlIDJ+Gc496mc4Gc5qq71s219p72CS3YWPJKO5zGQ3+6MkV41ruuTeLfFMkUDumnW7bYVJzhR/EfUnrXUaXpdpFECsO5u7NyTV+fTLS5Tc0Khx0deGH0NT+HpZpYruxuHZntWUo/qjZxn8qnmBUn5vzqv4teHRtHure2gMEEsLTRyQp8rSOduCcccD9a8s8LRtJeXaxtiQkFfwrq4G1W2fdKSIgR/ETWtfW95evCbaYRoQCwJPP61s+HLWWG/uoERZZWtkZstjOGOP51dltblpJA1syBTxlgc/lXJfFC9lg0qwsHD+YxMjvI3zH2x2615RpOryaXqouSSYC+HUDt616a9+NRtI3tmWZSM7d2CPer9rNdLBvnVI4Y15dm6AVo+Cdbt9Q8TXpSVRGLYKjMcb8N2/Ou6mGeT0rwbxzfvfauxeQuUULyc4rj4rMNuUjOTXd6FYSJbAY4X7pzg0zxKlxLbiBpHEZPIB4NW/B9xBpt95pgModcYIx+Vdy3iOxlh8iRXhbOGKkqQw9q8au91xcFmydxyams4MkgDJya7azj8q3QjpjpWDrs5EwTzB0JwozTNIvB9qtlPLAc5PWur110tr/fJt8qVVdc+4/xr//Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=48x48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN: sad (34.13%)\n",
      "CatBoost: happy (55.33%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitry\\AppData\\Local\\Temp\\ipykernel_36164\\4271670898.py:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(f\"CatBoost: {classes[int(cb_pred[0])]} ({cb_conf:.2%})\")\n"
     ]
    }
   ],
   "source": [
    "def predict_photo(image_path):\n",
    "    img = Image.open(image_path).convert('L').resize((48, 48))\n",
    "    display(img) \n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "    img_tensor = torch.from_numpy(img_array).unsqueeze(0).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        # CNN\n",
    "        cnn_output = model(img_tensor)\n",
    "        cnn_probs = torch.nn.functional.softmax(cnn_output, dim=1)\n",
    "        cnn_conf, cnn_pred = torch.max(cnn_probs, 1)\n",
    "        \n",
    "        # CatBoost\n",
    "        img_flat = img_array.flatten().reshape(1, -1)\n",
    "        cb_pred = catboost_model.predict(img_flat) \n",
    "        cb_proba = catboost_model.predict_proba(img_flat)\n",
    "        cb_conf = np.max(cb_proba)\n",
    "    \n",
    "    print(f\"CNN: {classes[cnn_pred.item()]} ({cnn_conf.item():.2%})\")\n",
    "    print(f\"CatBoost: {classes[int(cb_pred[0])]} ({cb_conf:.2%})\")\n",
    "\n",
    "predict_photo(\"../my_photo.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5cfcd6-b7c8-46c6-a899-75d3068f7869",
   "metadata": {},
   "source": [
    "Ожидалось happy btw.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2907503-e187-4c71-858e-c4d182f211cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN:\n",
      " angry: 8.24%\n",
      " disgust: 0.08%\n",
      " fear: 8.19%\n",
      " happy: 14.64%\n",
      " neutral: 33.62%\n",
      " sad: 34.13%\n",
      " surprise: 1.09%\n",
      "\n",
      "CatBoost:\n",
      " angry: 13.32%\n",
      " disgust: 0.53%\n",
      " fear: 10.98%\n",
      " happy: 55.33%\n",
      " neutral: 6.42%\n",
      " sad: 12.42%\n",
      " surprise: 0.99%\n"
     ]
    }
   ],
   "source": [
    "def predict_all(image_path):\n",
    "    img = Image.open(image_path).convert('L').resize((48, 48))\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "    # CNN\n",
    "    img_tensor = torch.from_numpy(img_array).unsqueeze(0).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        cnn_probs = torch.softmax(model(img_tensor), dim=1)[0]\n",
    "    print(\"CNN:\")\n",
    "    for i, em in enumerate(classes):\n",
    "        print(f\" {em}: {100*cnn_probs[i]:.2f}%\")\n",
    "\n",
    "\n",
    "    # CatBoost\n",
    "    cb_probs = catboost_model.predict_proba(img_array.reshape(1, -1))[0]\n",
    "    print(\"\\nCatBoost:\")\n",
    "    for i, em in enumerate(classes):\n",
    "        print(f\" {em}: {100*cb_probs[i]:.2f}%\")\n",
    "\n",
    "predict_all(\"../my_photo.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990ba9f6-d79b-45d3-8393-467405801048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ae094-5b2d-4c0f-82e1-4b6259608234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
